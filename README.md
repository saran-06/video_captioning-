VIDEO-CAPTIONING
      In todayâ€™s world, video captioning is highly used in various applications for hearing
impaired and, more specifically, specially abled persons. We are using the whisper library.
Whisper is an automation speech recognition. Whisper can transcribe large audio files with
human-level performance. We are using a gradio library which helps to create customizable
UI components within colab , jupyter notebook or scripts and around TensorFlow or PyTorch
models, or even arbitrary Python functions.The video caption paradigm, in a nutshell, is
providing a video to the algorithm and converting mp4 video to mp2 video using ffmpeg.
Using IPython.display importing audio file, which will convert the mp3 video into audio. We are
translating the audio into text. It will provide the language in which the video was present.
Getting the description of the video in the form of readable text. Base64 is used to encode
binary data as printable text. This allows you to transport binary over protocols or mediums
that cannot handle binary data formats and require simple text. Again converting video to mp4.
Based on the video speed we convert the text.Text will be generated based on video speed. We
are merging the text. CNNs are pretrained on object and/or action recognition tasks and used to
encode video-level features. The decoder is then optimized on such static features to generate the
video's description. This disjoint setup is arguably sub-optimal for input (video) to output
(description) mapping. In this work, we propose to optimize both encoder and decoder
simultaneously in an end-to-end fashion.


